import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.PrintWriter;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.LinkedList;
import java.util.List;

import cern.jet.random.engine.MersenneTwister;


public class Main {
	

	private static final int CHUNKSIZE = 1600;
	private static final int L = 6;
	private static final int K = 50;
	private static final int TESTNUMBER = 5;
	private static final int ITERATIONS = 150;
	private static final double PERCENTUNLABELLED = 0.9;
	private static final String OUTPUTGRAPHNAME = "output/4classPs";
	private static final String GRAPHTITLE = "Power Stream with four classes";
	private static final boolean SYNTHETIC = false;
	private static final int ERRORINTERVAL = 25;

	//This is not a complete list of the constants used in the algorithm.
	//Other parameters associated with specific parts of the algorithm are in those class files.
	
	private static final int WARMUP = 3;							//The number of data chunks processed before we start doing certain operations. 
	private static final int CHUNKSIZE = 1000;						//chunksize of data stream
	private static final int L = 6;									//Size of ensemble
	private static final int K = 50;								//Number of macro-clusters
	private static final int TESTNUMBER = 20;						//Number of times tests are repeated, due to non-deterministic nature of algorithm
	private static final int ITERATIONS = 150;						//Number of iterations for single test
	private static final int CONTIGMICRO = 3;						//Number of contiguous chunks which contribute to  
	private static final double PERCENTUNLABELLED = 0.9;			//Proportion of data which is unlabelled
	private static final double SIGMA = 0.25;						//Sigma for label propagation
	private static final String OUTPUTGRAPHNAME = "output/KDDPost";	//Root name of fileoutput. Will generate line, box and raw output files based on this filename.
	
	//If we use a synthetic dataset, we will not attempt to read from file.
	private static final boolean SYNTHETIC = true;

	
	//Otherwise, the file we are reading from needs to be specified at the beginning of the ReaSC function.
	//The graphing tools and test runs are not currently decoupled, i.e. graphs are generated by teh graphing unit after each test. 
	//However, if you wish to regraph information the .raw text file contains all the information that the current graphing unit uses. 
	
	private static final String GRAPHTITLE1 = "KDD Cup Problem";
	private static final String GRAPHTITLE2 = "Forest Cover Problem";
	private static final String GRAPHTITLE3 = "Power Stream problem, two classes";
	private static final String GRAPHTITLE5 = "Binary Sensor Problem";
	
	private static final int ERRORINTERVAL = 25;	//The error interval is how many chunks we process before we record the variance of a run.

	private static final String FILE1 = "input/kddcup.data_10_percent_corrected";
	private static final String FILE2 = "input/covtype.data";
	private static final String FILE3 = "input/2classpsbig.csv";

	private static final String FILE4 = "input/4classps.csv";

	private static final String FILE5 = "input/bigbinarysensor.csv";


	/*The functionality of this main function is to initialise the data structures which record output from the runs of the algorithm,
	run the algorithm for a single test (where the method of */
	
	public static void main(String[] args) throws IOException {
		
		//Hack associated with one of the comparators not being compatible with the timsort used in Java 1.7 onwards. 
		System.setProperty("java.util.Arrays.useLegacyMergeSort", "true");
		
		//The .raw file contains the arrays generated here for post processing if necessary.
		PrintWriter writer = new PrintWriter(OUTPUTGRAPHNAME + "raw.txt", "UTF-8");
		
		//The error arrays contain a number of variances measures taken throughout the run of the algorithm based on the ERRORINTERVAL
		
		LinkedList<Double> error1 = new LinkedList<Double>();
		LinkedList<Double> error2 = new LinkedList<Double>();
		LinkedList<Double> error3 = new LinkedList<Double>();
		
		//The cq arrays contain a subset of the cluster measures on each of the runs, so that we can get a rough measure of cluster quality.
		ArrayList<Double> cq1 = new ArrayList<Double>();
		ArrayList<Double> cq2 = new ArrayList<Double>();
		ArrayList<Double> cq3 = new ArrayList<Double>();
		
		//Control run: No outlier removal or cluster weighing
		LinkedList<Double> results1 = singleTest(false, false, error1, cq1);	
		writeToRaw(results1, writer, -1);
		writeToRaw(error1, writer, -1);
		writeToRaw(cq1, writer, -1);
		
		//Variation: Just outlier removal
		LinkedList<Double> results2 = singleTest(true, false, error2, cq2);
		writeToRaw(results2, writer, -1);
		writeToRaw(error2, writer, -1);
		writeToRaw(cq2, writer, -1);
		
		//Variation: Just cluster weighting
		LinkedList<Double> results3 = singleTest(false,true, error3, cq3);	
		writeToRaw(results3, writer, -1);
		writeToRaw(error3, writer, -1);
		writeToRaw(cq3, writer, -1);

		writer.close();
		
		//graph results
		Graphing.exportGraph(results1, results2, results3, error1, error2, error3, cq1, cq2,cq3 ,ERRORINTERVAL, GRAPHTITLE5, OUTPUTGRAPHNAME);
	}
	
	/* The purpose of this function is to run a test with a single method of cluster evaluation, and then process the results of the 
	 * various runs into a single line graph.*/
	private static LinkedList<Double> singleTest(boolean removal, boolean ratingCluster, LinkedList<Double> error, ArrayList<Double> cq) throws IOException{
		
		//The results vector contains the average cumulative accuracy for each chunk over all runs. After the first run,
		//the average is the first run.
		LinkedList<Double> results = ReaSC(L, K, PERCENTUNLABELLED, CHUNKSIZE, SYNTHETIC, removal, ratingCluster, cq);
		
		//The currentResult vector has the average cumulative accuracy for each chunk over a single run.
		LinkedList<Double> currentResult;
		
		//array for storing all the error arrays for each run
		LinkedList<LinkedList<Double>> allErrorStore = new LinkedList<LinkedList<Double>>();
		
		//storage for the errors for a single run
		LinkedList<Double> currentErrorStore = new LinkedList<Double>();
		
		//We have different error intervals for each variation so the error bars don't overlap on the graph.
		int interval = 0;
		if(ratingCluster){
			interval = ERRORINTERVAL/3;
		}
		if(removal){
			interval = 2*ERRORINTERVAL/3;
		}
		
		//Record the cumulative accuracy after a certain number of chunks have been processed. 
		for(int j = 0; j < results.size(); j++){
			if(j%ERRORINTERVAL == interval){
				currentErrorStore.add(results.get(j));
			}
		}
		allErrorStore.add(currentErrorStore);
		
		//Perform the remaining runs. 
		for(int i = 1; i < TESTNUMBER; i++){
			
			System.out.println("Test " + i + " complete");
			
			//Perform run
			currentResult = ReaSC(L, K, PERCENTUNLABELLED, CHUNKSIZE, SYNTHETIC, removal,ratingCluster, cq);
			
			//After the run, add the accuracy to the total accuracy vector. Also, process the error. 
			currentErrorStore = new LinkedList<Double>();
			for(int j = 0; j < currentResult.size(); j++){
				results.set(j, results.get(j)+currentResult.get(j));
				if(j%ERRORINTERVAL == interval){
					currentErrorStore.add(currentResult.get(j));
				}
			}
			allErrorStore.add(currentErrorStore);
		}
		
		//Calculate the total accuracy over all runs by dividing the accuracy over those runs by the number of runs.
		LinkedList<Double> avs = new LinkedList<Double>();
		for(int j = 0; j < results.size(); j++){
			results.set(j,results.get(j)/TESTNUMBER);
			if(j%ERRORINTERVAL == interval){
				avs.add(results.get(j));
				System.out.println("Average of " + j + ": " + results.get(j));
			}
		}

		//Calculate the standard deviation for each of the error intervals by comparing the variation to the average in the standard way.
		for(int i = 0; i < allErrorStore.size(); i++){
			
			LinkedList<Double> temp = allErrorStore.get(i);
			for(int j = 0; j < allErrorStore.get(i).size(); j++){
				//store now contains array of (value -avs)^2
				allErrorStore.get(i).set(j, (temp.get(j) - avs.get(j))*(temp.get(j) - avs.get(j)));
				System.out.println("Setting " + i + " " + j + " to be" + allErrorStore.get(i).get(j));
			}
			
		}
		
		//Second part of the standard deviation calculation.
		for(int j = 0; j < allErrorStore.get(0).size(); j++){
			error.add(0.0);
			for(int i = 0; i < allErrorStore.size(); i++){
				error.set(j,error.get(j)+allErrorStore.get(i).get(j));
			}
			error.set(j, Math.sqrt(error.get(j)));
		}
		return results;
	}
	
	//output a single vector to a raw file.
	public static void writeToRaw(List<Double> list, PrintWriter p, int test){
		if(test > -1){
			p.print(test + ":");
		}
		for(Double d: list){
			p.print(d + " ");
		}
		p.println();
	}
	
	//The core computation in the stream mining algorithm.
	public static LinkedList<Double> ReaSC(int l, int k, double percentUnlabelled, int chunSize, boolean synthetic, boolean removal,boolean ratingCluster, ArrayList<Double> cqsummary) throws IOException{
		
		//Get file handle if we are reading a dataset from the file.
		BufferedReader br;
		if(!synthetic){
			br = new BufferedReader(new FileReader(FILE5));
		}else{
			br = null;
		}
		
		//A mersenne twister for random number generation throughout the algorithm.
		MersenneTwister twist = new MersenneTwister(new java.util.Date());
		
		//Number of classes. 
		int c;
		
		//Create the ensemble
		Ensemble ens = new Ensemble(l,k);
		
		int vectorLength = 0;	//the number of fields to be read from the dataset.
		int chunkSize = chunSize;	//chunksize
		
		//Vector for recording the cumulative accuracy after each chunk is processed.
		LinkedList<Double> percentArray = new LinkedList<Double>();
		
		//Read the number of fields from the csv file or if the data is synthetic use a preset value. 
		if(!synthetic){
			try {
				vectorLength = br.readLine().split(",").length;
				System.out.println("Length: " + vectorLength);
			
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
		}else{
			vectorLength = SYNTHETICLENGTH;
		}
		
		//Create the class which turns the raw data into data points.
		DataProcessor d = new DataProcessor(vectorLength, percentUnlabelled,br,synthetic,twist);
		
		int iterations = 0;
		while(iterations < ITERATIONS){
			
			//Read in a certain amount of data and create a data chunk. Includes the labelling of some of the data.
			DataChunk chunk = new DataChunk(chunkSize, d,twist, percentUnlabelled);
			
			//Simple counting function which records information on class distribution of labelled data.
			ens.countNewDataPoints(chunk.getTrainingData());
			
			
			//If the data chunk is not full, i.e. we ran out of instances from file, we stop.
			if(chunk.getDataPointArray().size() < chunkSize){
				return percentArray;
			}
			
			//If new classes have been seen, we need to increase the capacity of certain data structures in the ensemble. 
			c = d.getSeenClasses();
			System.out.println("Seen classes: " + c);
			ens.expandClasses(c);
			
			//Make tentative predictions of class of datapoint for microcluster splitting phase.
			if(iterations > WARMUP){
				ens.predictChunkForClustering(chunk);
			}
			//After a certain number of iterations, we begin recording cumulative accuracy.
			if(iterations > l+WARMUP){
				ens.predictChunk(chunk);
				percentArray.add(ens.getAccuracy());
				System.out.println("After " + iterations +" iterations, the accuracy is:" + ens.getAccuracy());
				System.out.println();
			}
	
			//Build model
			Model m = new Model(twist, chunk,k, c, ens.getClassCounter(), ens.getTotalPoints(), iterations, removal, ratingCluster);
			
			//take summary of the quality of the microcluster in this model.
			ArrayList<Double> loopSummary = m.getLoopSummary();
			for(Double dd: loopSummary){
				cqsummary.add(dd);
			}
			
			//perform label propagation 
			if(iterations > WARMUP){
				m.propagateLabels(CONTIGMICRO, SIGMA, ens.getContig());
			}
			
			//ensemble is now ready to have the new model added.  
			ens.addModel(m);
			iterations++;
			
		}
		return percentArray;
	}

}

